---
title: "Metab_In"
author: "Andrew Blinn"
date: "1/5/2022"
output: html_document
---

The purpose of this script is to take in sensor data from the CURB Google Drive
and output processed files ready for modeling. Annotated meta data including any 
alterations to sensor check times made based on evidence of out-of-sensor or
flagged as having poor data quality. Output files are saved locally in addition
to being uploaded to Google Drive. These local files can then be uploaded to the
GACRC at a later time. Alternative output formats for data sharing are possible
but not yet supported.

When prompted, log in to a Google account to access CURB (new! Use me!).

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)

# Install packages as necessary

library(streamMetabolizer) # needed for simulated light data and conversions
library(dataRetrieval) # retrieve hydro data from USGS
# library(googledrive) # download and upload with CURB drive folder
library(tidyverse) # plotting with ggplot and using dplyr tools
library(readxl) # read excel files
library(plotly) # fancy plots for reports
library(zoo) # for linear interpolation of small gaps in time series
library(dtw) # dynamic time warping of time series from different regions

```


```{r metadata, include=FALSE}

# read in the metadata

my_streams <- read_csv("my_streams.csv")

```

Read in data from three streams in Atlanta, GA, USA
```{r Atlanta}

for(i in 1:nrow(my_streams)){
  
  # focus on one city at a time
  if(my_streams$city[i]=="ATL"){
    
    # pull all relevant data from USGS
    atlanta_data <- readNWISuv(my_streams$gage[i], 
                            c("00060", # discharge (f^3 s^-1)
                              "00065", # gage height (f)
                              "00010", # temperature (C)
                              "00095", # specific conductance (uS/cm @ 25C)
                              "00300", # dissolved oxygen (mg L^-1)
                              "00301"), # dissolved oxygen sat (% sat)
                            tz = "EST",
                            startDate = "2020-09-30", 
                            endDate = "2022-10-01") %>%
      
      mutate(date = as.Date(dateTime),
             temp.water = X_00010_00000, # degrees C
             DO.obs = X_00300_00000, # mg per L
             discharge = ifelse(i==11, 
                                # SR24 is missing discharge data so use depth
                                # coverted to meters before using equation
                                (((X_00065_00000/3.28084)/0.409)^(1/0.294)), 
                                # otherwise covert existing data
                                X_00060_00000/35.315), # cfs to cms
             depth = X_00065_00000/3.28084) # feet to meters
    
    # read weather data and add to existing table
    # from https://meteostat.net/en/place/us/atlanta?s=72219&t=2019-01-01/2021-12-31
    atlanta_data <- merge(atlanta_data, read_csv("ATL_weather.csv"), by="date")
    
    # calculate solar time using site longitude
    atlanta_data$solar.time <- calc_solar_time(atlanta_data$dateTime,
                                               as.numeric(my_streams$longitude[i]))
    
    # simulate light in PAR (umol m^-2 s^-1)
    atlanta_data$light <- calc_light(atlanta_data$solar.time, 
                                    as.numeric(my_streams$latitude[i]),
                                    as.numeric(my_streams$longitude[i]))
    
    # calculate 100% DO saturation concentration (mg/L)
    atlanta_data$DO.sat <- calc_DO_sat(atlanta_data$temp.water, # degrees C
                                       atlanta_data$pres) # millibars
    
    # cleaning steps
    atlanta_data <- atlanta_data %>% 
      
      # sort by solar.time
      arrange(solar.time) %>%
      
      # remove any repeat time steps
      distinct(solar.time, .keep_all = T) %>%
      
      # keep only columns needed for modeling
      select(solar.time, light, temp.water, DO.obs, DO.sat, depth, discharge)
    
    # filter to keep full model days
    clean_data <- mm_filter_valid_days(atlanta_data)$data[,2:8]
    
    write.csv(clean_data, 
              file = paste0(my_streams$CURB_ID[i],"_metab_input.csv"), 
              row.names=F)
    
    rm(atlanta_data)
    rm(clean_data)
  }
}

```

Read in data from three streams in Boston, MA, USA
```{r Boston}

boston_data <- read_excel("boston_sensors_clean.xlsx", na = c("","NA")) %>%
  rename(DO.obs = do_mgl, dateTime = datetime, temp.water = temp_c)

boston_data$dateTime <- as.POSIXct(boston_data$dateTime, tz = "EST", 
                               format = "%m/%d/%y %H:%M")


for(i in 1:nrow(my_streams)){
  
  # focus on one city at a time - starting alphabetically with Boston, MA, USA
  if(my_streams$city[i]=="BOS"){
    # split data file into separate frames for each stream site in this city
    boston_stream_data <- boston_data[boston_data$site==my_streams$streamname[i],]
    
    # calculate solar time using site longitude
    boston_stream_data$solar.time <- calc_solar_time(boston_stream_data$dateTime, 
                                              as.numeric(my_streams$longitude[i]))
    
    # simulate light in PAR (umol m^-2 s^-1)
    boston_stream_data$light <- calc_light(boston_stream_data$solar.time, 
                                    as.numeric(my_streams$latitude[i]),
                                    as.numeric(my_streams$longitude[i]))
    
    # calculate 100% DO saturation concentration (mg/L)
    boston_stream_data$DO.sat <- calc_DO_sat(boston_stream_data$temp.water, 
                                      (boston_stream_data$BP_mmHg)*1.33322)
    
    # and discharge
    boston_hydro <- readNWISuv(my_streams$gage[i], 
                            c("00060", "00065"), 
                            tz = "EST",
                            startDate = my_streams$start[i], 
                            endDate = my_streams$end[i])

    boston_hydro$discharge <- boston_hydro$X_00060_00000/35.315
    boston_hydro$depth <- boston_hydro$X_00065_00000/3.28084
    
    boston_stream_data <- merge(boston_stream_data, boston_hydro[,c(3,9,10)], all.x = T)
    boston_stream_data <- distinct(boston_stream_data, solar.time, .keep_all = T)
    
    if(my_streams$SN[i]=="AB"){
      boston_stream_data <- boston_stream_data[-37614,]
    }

    if(my_streams$SN[i]=="SB"){
      boston_stream_data <- boston_stream_data[-37630,]
    }
    
    
    boston_stream_data$discharge <- na.approx(boston_stream_data$discharge)
    boston_stream_data$depth <- na.approx(boston_stream_data$depth)

    boston_stream_data <- distinct(boston_stream_data, solar.time, .keep_all = T)
    
    clean_data <- mm_filter_valid_days(boston_stream_data)$data
    
    write.csv(clean_data[,c(13, 8, 15, 17, 5, 14, 16)], 
              file = paste0(my_streams$streamname[i],"_metab_input.csv"), 
              row.names=F)
    rm(boston_stream_data)
    rm(boston_hydro)
    rm(clean_data)
  }
}

```

Read in data from five streams in Miami, FL, USA
```{r Miami}

miami_data <- read_excel("miami_sensors_clean.xlsx", na = c("","NA")) %>%
  rename(DO.obs = do_mgl, dateTime = datetime, temp.water = temp_c)

miami_data$dateTime <- as.POSIXct(miami_data$dateTime, tz = "EST", 
                               format = "%m/%d/%y %H:%M")


for(i in 1:nrow(my_streams)){
  
  # focus on one city at a time - starting alphabetically with miami, MA, USA
  if(my_streams$city[i]=="SLC"){
    # split data file into separate frames for each stream site in this city
    miami_stream_data <- miami_data[miami_data$site==my_streams$streamname[i],]
    
    # calculate solar time using site longitude
    miami_stream_data$solar.time <- calc_solar_time(miami_stream_data$dateTime, 
                                              as.numeric(my_streams$longitude[i]))
    
    # simulate light in PAR (umol m^-2 s^-1)
    miami_stream_data$light <- calc_light(miami_stream_data$solar.time, 
                                    as.numeric(my_streams$latitude[i]),
                                    as.numeric(my_streams$longitude[i]))
    
    # calculate 100% DO saturation concentration (mg/L)
    miami_stream_data$DO.sat <- calc_DO_sat(miami_stream_data$temp.water, 
                                      (miami_stream_data$BP_mmHg)*1.33322)
    
    # and discharge
    miami_hydro <- readNWISuv(my_streams$gage[i], 
                            c("00060", "00065"), 
                            tz = "EST",
                            startDate = my_streams$start[i], 
                            endDate = my_streams$end[i])

    miami_hydro$discharge <- miami_hydro$X_00060_00000/35.315
    miami_hydro$depth <- miami_hydro$X_00065_00000/3.28084
    
    miami_stream_data <- merge(miami_stream_data, miami_hydro[,c(3,9,10)], all.x = T)
    miami_stream_data <- distinct(miami_stream_data, solar.time, .keep_all = T)
    
    if(my_streams$SN[i]=="AB"){
      miami_stream_data <- miami_stream_data[-37614,]
    }

    if(my_streams$SN[i]=="SB"){
      miami_stream_data <- miami_stream_data[-37630,]
    }
    
    
    miami_stream_data$discharge <- na.approx(miami_stream_data$discharge)
    miami_stream_data$depth <- na.approx(miami_stream_data$depth)

    miami_stream_data <- distinct(miami_stream_data, solar.time, .keep_all = T)
    
    clean_data <- mm_filter_valid_days(boston_stream_data)$data
    
    write.csv(clean_data[,c(13, 8, 15, 17, 5, 14, 16)], 
              file = paste0(my_streams$streamname[i],"_metab_input.csv"), 
              row.names=F)
    rm(miami_stream_data)
    rm(miami_hydro)
    rm(clean_data)
  }
}

```

Read in data from five streams in Portland, OR, USA
```{r Portland}

for(i in 1:nrow(my_streams)){
  
  # focus on one city at a time
  if(my_streams$city[i]=="PDX"){
    
    # pull all relevant data from USGS
    portland_data <- readNWISuv(my_streams$gage[i], 
                            c("00060", # discharge (f^3 s^-1)
                              "00065", # gage height (f)
                              "00010", # temperature (C)
                              "00095", # specific conductance (uS/cm @ 25C)
                              "00300", # dissolved oxygen (mg L^-1)
                              "00301"), # dissolved oxygen sat (% sat)
                            tz = "PST",
                            startDate = "2019-10-01", 
                            endDate = "2022-09-30") %>%
      
      mutate(date = as.Date(dateTime),
             temp.water = X_00010_00000, # degrees C
             DO.obs = X_00300_00000, # mg per L
             discharge = X_00060_00000/35.315, # cfs to cms
             depth = X_00065_00000/3.28084) # feet to meters
    
    # read weather data and add to existing table
    # from https://meteostat.net/en/place/us/portland?s=72219&t=2019-01-01/2021-12-31
    portland_data <- merge(portland_data, read_csv("PDX_weather.csv"), by="date")
    
    # calculate solar time using site longitude
    portland_data$solar.time <- calc_solar_time(portland_data$dateTime,
                                               as.numeric(my_streams$longitude[i]))
    
    # simulate light in PAR (umol m^-2 s^-1)
    portland_data$light <- calc_light(portland_data$solar.time, 
                                    as.numeric(my_streams$latitude[i]),
                                    as.numeric(my_streams$longitude[i]))
    
    # cleaning steps
    portland_data <- portland_data %>% 
      
      # sort by solar.time
      arrange(solar.time) %>%
      
      # remove any repeat time steps
      distinct(solar.time, .keep_all = T) %>%
      
      # keep only columns needed for modeling
      select(solar.time, light, temp.water, DO.obs, depth, discharge)
    
    # linear interpolation of 30-minute frequency water quality data to 15-min
    zoo_temp.water_data <- zoo(portland_data$temp.water, portland_data$solar.time)
    
    portland_data$temp.water <- na.approx(zoo_temp.water_data, 
                                          xout = portland_data$solar.time, 
                                          rule = 2)
    
    rm(zoo_temp.water_data)
    
    # linear interpolation of 30-minute frequency water quality data to 15-min
    zoo_DO.obs_data <- zoo(portland_data$DO.obs, portland_data$solar.time)
    
    portland_data$DO.obs <- na.approx(zoo_DO.obs_data, 
                                          xout = portland_data$solar.time,
                                          rule = 2)
    
    # calculate 100% DO saturation concentration (mg/L)
    portland_data$DO.sat <- calc_DO_sat(portland_data$temp.water, # degrees C
                                       portland_data$pres) # millibars
    
    # filter to keep full model days
    clean_data <- mm_filter_valid_days(portland_data)$data[,2:8]
    
    write.csv(clean_data, 
              file = paste0(my_streams$CURB_ID[i],"_metab_input.csv"), 
              row.names=F)
    
    rm(portland_data)
    rm(clean_data)
  }
}

```

Read in data from five streams in Salt Lake City, UT, USA
```{r SaltLake}

saltlake_data <- read_excel("saltlake_sensors_clean.xlsx", na = c("","NA")) %>%
  rename(DO.obs = do_mgl, dateTime = datetime, temp.water = temp_c)

saltlake_data$dateTime <- as.POSIXct(saltlake_data$dateTime, tz = "EST", 
                               format = "%m/%d/%y %H:%M")


for(i in 1:nrow(my_streams)){
  
  # focus on one city at a time - starting alphabetically with saltlake, MA, USA
  if(my_streams$city[i]=="SLC"){
    # split data file into separate frames for each stream site in this city
    saltlake_stream_data <- saltlake_data[saltlake_data$site==my_streams$streamname[i],]
    
    # calculate solar time using site longitude
    saltlake_stream_data$solar.time <- calc_solar_time(saltlake_stream_data$dateTime, 
                                              as.numeric(my_streams$longitude[i]))
    
    # simulate light in PAR (umol m^-2 s^-1)
    saltlake_stream_data$light <- calc_light(saltlake_stream_data$solar.time, 
                                    as.numeric(my_streams$latitude[i]),
                                    as.numeric(my_streams$longitude[i]))
    
    # calculate 100% DO saturation concentration (mg/L)
    saltlake_stream_data$DO.sat <- calc_DO_sat(saltlake_stream_data$temp.water, 
                                      (saltlake_stream_data$BP_mmHg)*1.33322)
    
    # and discharge
    saltlake_hydro <- readNWISuv(my_streams$gage[i], 
                            c("00060", "00065"), 
                            tz = "EST",
                            startDate = my_streams$start[i], 
                            endDate = my_streams$end[i])

    saltlake_hydro$discharge <- saltlake_hydro$X_00060_00000/35.315
    saltlake_hydro$depth <- saltlake_hydro$X_00065_00000/3.28084
    
    saltlake_stream_data <- merge(saltlake_stream_data, saltlake_hydro[,c(3,9,10)], all.x = T)
    saltlake_stream_data <- distinct(saltlake_stream_data, solar.time, .keep_all = T)
    
    if(my_streams$SN[i]=="AB"){
      saltlake_stream_data <- saltlake_stream_data[-37614,]
    }

    if(my_streams$SN[i]=="SB"){
      saltlake_stream_data <- saltlake_stream_data[-37630,]
    }
    
    
    saltlake_stream_data$discharge <- na.approx(saltlake_stream_data$discharge)
    saltlake_stream_data$depth <- na.approx(saltlake_stream_data$depth)

    saltlake_stream_data <- distinct(saltlake_stream_data, solar.time, .keep_all = T)
    
    clean_data <- mm_filter_valid_days(boston_stream_data)$data
    
    write.csv(clean_data[,c(13, 8, 15, 17, 5, 14, 16)], 
              file = paste0(my_streams$streamname[i],"_metab_input.csv"), 
              row.names=F)
    rm(saltlake_stream_data)
    rm(saltlake_hydro)
    rm(clean_data)
  }
}

```

Now go run the input files through the metabolism model and come back!


Load list of .RData files and compile output into single .csv
```{r Output}

for(i in 1:nrow(my_streams)){
  
  # Load model objects from metabolism.Rmd
  load(file = paste0("D:/CURB Model Objects/", 
                    my_streams$SN[my_streams$streamname==stream_name], "/",filelist[i]))
  
  # Merge daily estimates, bounds, deviation, and modeling error Rhat values to single dataframe
  daily_metab <- data.frame(date = as.Date(mm@fit[["daily"]][["date"]]),
                       Site =  my_streams$SN[i],
                       
                       GPP = mm@fit[["daily"]][["GPP_daily_50pct"]],
                       GPP.lower = mm@fit[["daily"]][["GPP_daily_2.5pct"]],
                       GPP.upper = mm@fit[["daily"]][["GPP_daily_97.5pct"]],
                       GPP.daily.sd = mm@fit[["daily"]][["GPP_daily_sd"]],
                    
                       ER = mm@fit[["daily"]][["ER_daily_50pct"]],
                       ER.lower = mm@fit[["daily"]][["ER_daily_2.5pct"]],
                       ER.upper = mm@fit[["daily"]][["ER_daily_97.5pct"]],
                       ER.daily.sd = mm@fit[["daily"]][["ER_daily_sd"]],
                       
                       K600.daily = mm@fit[["daily"]][["K600_daily_50pct"]],
                       K600.daily.sd = mm@fit[["daily"]][["K600_daily_sd"]],
                       GPP_daily_n_eff = mm@fit[["daily"]][["GPP_daily_n_eff"]],
                       GPP_daily_Rhat = mm@fit[["daily"]][["GPP_daily_Rhat"]],
                       ER_daily_n_eff = mm@fit[["daily"]][["ER_daily_n_eff"]],
                       ER_daily_Rhat = mm@fit[["daily"]][["ER_daily_Rhat"]],
                       K600_daily_n_eff = mm@fit[["daily"]][["K600_daily_n_eff"]],
                       K600_daily_Rhat = mm@fit[["daily"]][["K600_daily_Rhat"]],
                       
                       warnings = mm@fit[["daily"]][["warnings"]],
                       errors = mm@fit[["daily"]][["errors"]])
  
  # Save dataframe as table to append with each new chunk, saving column names on first iteration
  write.table(daily_metab, "StonyBrook_daily_metab.csv", 
              append = ifelse(i==1, F, T), row.names = F, 
              col.names = ifelse(i==1, T, F), sep = ",")
  
  # Remove the large model object and associated dataframe before proceeding
  rm(mm)
  rm(daily_metab)
}

rm(filelist)

```
Round up all the data tables to create a master table.
```{r}

metab_all <- read_csv("CURB_all_daily_metab.csv") %>% 
  mutate(year = lubridate::year(date))

metab_summary <- metab_all %>%
  group_by(Site, year) %>%
  summarize(GPP_mean = mean(GPP),
            GPP_Rhat_mean = mean(GPP_daily_Rhat),
            ER_mean = mean(ER),
            ER_Rhat_mean = mean(ER_daily_Rhat),
            K600_mean = mean(K600.daily),
            K600_Rhat_mean = mean(K600_daily_Rhat),)

# Create boxplots for each variable (x, y, z) by site and year
p_GPP <- ggplot(metab_all, aes(x = factor(year), y = GPP, fill = Site)) +
  geom_boxplot() +
  labs(title = "GPP by Site and Year", x = "Year", y = "GPP (g O2 m^-2 d^-1)")

p_ER <- ggplot(metab_all, aes(x = factor(year), y = ER, fill = Site)) +
  geom_boxplot() +
  labs(title = "ER by Site and Year", x = "Year", y = "ER (g O2 m^-2 d^-1)")

p_K600 <- ggplot(metab_all, aes(x = factor(year), y = K600.daily, fill = Site)) +
  geom_boxplot() +
  labs(title = "K600 by Site and Year", x = "Year", y = "K600 (g O2 m^-2 d^-1)")

p_GPP
p_ER
p_K600
```

```{r time series}

# Create boxplots for each variable (x, y, z) by site and year
t_GPP <- ggplot(metab_all, aes(x = date, y = GPP, col = Site)) +
  geom_smooth() +
  labs(title = "GPP by Site", x = "Date", y = "GPP (g O2 m^-2 d^-1)") +
  theme_bw()

t_ER <- ggplot(metab_all, aes(x = date, y = ER, col = Site)) +
  geom_smooth() +
  labs(title = "ER by Site", x = "Date", y = "ER (g O2 m^-2 d^-1)") +
  theme_bw()
  
t_K600 <- ggplot(metab_all, aes(x = date, y = K600.daily, col = Site)) +
  geom_smooth() +
  labs(title = "K600 by Site", x = "Date", y = "K600 (g O2 m^-2 d^-1)") +
  theme_bw()
  
t_GPP
t_ER
t_K600

```

